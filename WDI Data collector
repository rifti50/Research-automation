{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rifti50/wdi-data-collector?scriptVersionId=263113276\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:50:14.155892Z","iopub.execute_input":"2025-02-03T09:50:14.156248Z","iopub.status.idle":"2025-02-03T09:50:15.222204Z","shell.execute_reply.started":"2025-02-03T09:50:14.156212Z","shell.execute_reply":"2025-02-03T09:50:15.221235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Install necessary module","metadata":{}},{"cell_type":"code","source":"pip install wbdata pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:57:37.708536Z","iopub.execute_input":"2025-05-22T10:57:37.708827Z","iopub.status.idle":"2025-05-22T10:57:58.856155Z","shell.execute_reply.started":"2025-05-22T10:57:37.708802Z","shell.execute_reply":"2025-05-22T10:57:58.855012Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Collecting the U.S.'s data from 1990-2023 of the following metrices","metadata":{}},{"cell_type":"markdown","source":"1) GDP per capita (constant 2015 US $)\n2) GDP per person employed (constant 2017 PPP $)\n3) Individuals using the Internet (% of population)\n4) Renewable energy consumption (% of total final energy) \n5) Domestic credit to the private sector by banks (% of GDP)\n6) Adjusted savings: education expenditure (% of GNI)\n7) Fixed telephone subscriptions, mobile cellular subscriptions, and individuals utilizing the Internet (% of the population) produce an index and are cited as indicators of digitalization\n8) Labor force, total\n9) Foreign direct investment, net inflows (% of GDP)\n10) The ratio of the working population to the working age population (15 years and older)","metadata":{}},{"cell_type":"code","source":"# Install wbdata if not already installed\n#!pip install wbdata\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\n\n# Define country - USA only (use proper country code)\ncountries = ['US']  # United States (ISO 2-letter code)\n\n# Define indicators for the 10 measures requested\nindicators = {\n    'NY.GDP.PCAP.KD': 'GDP per capita (constant 2015 US$)',  # 1\n    'SL.GDP.PCAP.EM.KD': 'GDP per person employed (constant 2017 PPP $)',  # 2\n    'IT.NET.USER.ZS': 'Individuals using the Internet (% of population)',  # 3\n    'EG.FEC.RNEW.ZS': 'Renewable energy consumption (% of total final energy)',  # 4\n    'FD.AST.PRVT.GD.ZS': 'Domestic credit to private sector by banks (% of GDP)',  # 5\n    'NY.ADJ.AEDU.GN.ZS': 'Adjusted savings: education expenditure (% of GNI)',  # 6\n    'SL.TLF.TOTL.IN': 'Labor force, total',  # 8\n    'BX.KLT.DINV.WD.GD.ZS': 'Foreign direct investment, net inflows (% of GDP)',  # 9\n    'SL.EMP.TOTL.SP.NE.ZS': 'Employment to population ratio, 15+, total (%) (national estimate)'  # 10\n}\n\n# Digitalization indicators (for measure 7)\ndigitalization_indicators = {\n    'IT.TEL.FIXD.P2': 'Fixed telephone subscriptions (per 100 people)',\n    'IT.CEL.SETS.P2': 'Mobile cellular subscriptions (per 100 people)',\n    'IT.NET.USER.ZS': 'Individuals using the Internet (% of population)'\n}\n\n# Define the start and end years (updated to 1990-2023)\nstart_date = datetime(1990, 1, 1)\nend_date = datetime(2023, 12, 31)\n\n# Define output file path\noutput_path = \"USA_Indicators_1990_2023.xlsx\"\n\nprint(f\"Fetching World Bank data for USA from 1990 to 2023...\")\nprint(f\"Output file: {output_path}\")\n\n# Create an Excel writer\nwith pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n    sheet_written = False  # Flag to check if at least one sheet is written\n    \n    # 1. Handle the digitalization indicators to create an index (Measure 7)\n    print(\"\\n\" + \"=\"*60)\n    print(\"Processing Digitalization Index (Measure 7)\")\n    print(\"=\"*60)\n    \n    try:\n        # Fetch digitalization data\n        digital_data = {}\n        for code, name in digitalization_indicators.items():\n            print(f\"Fetching: {name}\")\n            try:\n                df = wbdata.get_dataframe(\n                    {code: name},\n                    country=countries,\n                    date=(start_date, end_date)\n                )\n                \n                if df is not None and not df.empty:\n                    # Reset index to get country and date as columns\n                    df = df.reset_index()\n                    \n                    # Handle different column structures\n                    if 'date' in df.columns:\n                        df['year'] = pd.to_datetime(df['date']).dt.year\n                    elif df.index.name == 'date':\n                        df = df.reset_index()\n                        df['year'] = pd.to_datetime(df['date']).dt.year\n                    \n                    # Ensure we have country column\n                    if 'country' not in df.columns:\n                        df['country'] = 'United States'\n                    \n                    # Select relevant columns\n                    cols_to_keep = ['country', 'year', name]\n                    available_cols = [col for col in cols_to_keep if col in df.columns]\n                    df = df[available_cols].sort_values('year')\n                    digital_data[name] = df\n                    print(f\"  ✓ Data fetched: {len(df)} records\")\n                else:\n                    print(f\"  ✗ No data available\")\n            except Exception as e:\n                print(f\"  ✗ Error: {str(e)}\")\n        \n        # Combine digitalization data and create index\n        if digital_data:\n            # Start with the first available dataset\n            combined_df = None\n            for name, df in digital_data.items():\n                if combined_df is None:\n                    combined_df = df.copy()\n                else:\n                    combined_df = pd.merge(combined_df, df, on=['country', 'year'], how='outer')\n            \n            if combined_df is not None:\n                # Calculate digitalization index\n                digital_cols = list(digitalization_indicators.values())\n                available_cols = [col for col in digital_cols if col in combined_df.columns]\n                \n                if available_cols:\n                    # Normalize each indicator to 0-1 scale\n                    for col in available_cols:\n                        col_data = combined_df[col].dropna()\n                        if len(col_data) > 0:\n                            min_val = col_data.min()\n                            max_val = col_data.max()\n                            if max_val > min_val:\n                                combined_df[f'{col}_normalized'] = (combined_df[col] - min_val) / (max_val - min_val)\n                            else:\n                                combined_df[f'{col}_normalized'] = 1.0\n                    \n                    # Calculate the digitalization index as average of normalized values\n                    normalized_cols = [f'{col}_normalized' for col in available_cols]\n                    combined_df['Digitalization_Index'] = combined_df[normalized_cols].mean(axis=1)\n                    \n                    # Prepare final dataframe\n                    final_cols = ['country', 'year'] + available_cols + ['Digitalization_Index']\n                    digital_final_df = combined_df[final_cols].sort_values(['country', 'year'])\n                    \n                    # Save digitalization data\n                    digital_final_df.to_excel(writer, sheet_name=\"Digitalization_Index\", index=False)\n                    sheet_written = True\n                    print(f\"✓ Digitalization Index created and saved ({len(digital_final_df)} records)\")\n                else:\n                    print(\"✗ No digitalization data available for index calculation\")\n    except Exception as e:\n        print(f\"✗ Error processing digitalization data: {str(e)}\")\n    \n    # 2. Handle all other indicators\n    print(\"\\n\" + \"=\"*60)\n    print(\"Processing Other Economic Indicators\")\n    print(\"=\"*60)\n    \n    for i, (indicator_code, indicator_name) in enumerate(indicators.items(), 1):\n        print(f\"\\n{i}. Fetching: {indicator_name}\")\n        print(f\"   Code: {indicator_code}\")\n        \n        try:\n            # Fetch data for the given country and indicator\n            df = wbdata.get_dataframe(\n                {indicator_code: indicator_name}, \n                country=countries, \n                date=(start_date, end_date)\n            )\n            \n            # If the dataframe is None or empty, skip writing it\n            if df is None or df.empty:\n                print(f\"   ✗ No data available\")\n                continue\n            \n            # Reset index to get date and country as columns\n            df = df.reset_index()\n            \n            # Handle date column processing\n            if 'date' in df.columns:\n                if not pd.api.types.is_datetime64_any_dtype(df['date']):\n                    df['date'] = pd.to_datetime(df['date'])\n                df['year'] = df['date'].dt.year\n                df = df.drop(columns=['date'])\n            elif df.index.name == 'date':\n                df['year'] = pd.to_datetime(df.index).year\n            \n            # Ensure we have country column\n            if 'country' not in df.columns:\n                df['country'] = 'United States'\n            \n            # Sort by country and year\n            df = df.sort_values(by=['year'])\n            \n            # Create a safe sheet name (Excel sheet names have character limits)\n            sheet_name = indicator_name.replace('(', '').replace(')', '').replace('%', 'pct')\n            sheet_name = sheet_name.replace(',', '').replace(':', '')[:31]\n            \n            # Save data to a sheet\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n            sheet_written = True\n            print(f\"   ✓ Data saved ({len(df)} records)\")\n            \n        except Exception as e:\n            print(f\"   ✗ Error: {str(e)}\")\n            continue\n    \n    # 3. Create a summary sheet with all indicators combined\n    print(\"\\n\" + \"=\"*60)\n    print(\"Creating Combined Summary Sheet\")\n    print(\"=\"*60)\n    \n    try:\n        # Fetch all data again for the summary\n        all_data = {}\n        \n        # Add regular indicators\n        for code, name in indicators.items():\n            try:\n                df = wbdata.get_dataframe(\n                    {code: name},\n                    country=countries,\n                    date=(start_date, end_date)\n                )\n                \n                if df is not None and not df.empty:\n                    df = df.reset_index()\n                    if 'date' in df.columns:\n                        df['year'] = pd.to_datetime(df['date']).dt.year\n                        df = df[['year', name]].sort_values('year')\n                        all_data[name] = df.set_index('year')[name]\n            except:\n                pass\n        \n        # Add digitalization index if available\n        if 'digital_final_df' in locals() and not digital_final_df.empty:\n            digital_summary = digital_final_df.set_index('year')['Digitalization_Index']\n            all_data['Digitalization Index'] = digital_summary\n        \n        # Combine all data\n        if all_data:\n            summary_df = pd.DataFrame(all_data)\n            summary_df.index.name = 'Year'\n            summary_df = summary_df.reset_index().sort_values('Year')\n            \n            # Save summary\n            summary_df.to_excel(writer, sheet_name=\"Summary_All_Indicators\", index=False)\n            print(f\"✓ Summary sheet created with {len(summary_df)} years of data\")\n        \n    except Exception as e:\n        print(f\"✗ Error creating summary: {str(e)}\")\n    \n    # If no data was written, create a dummy sheet to prevent the error\n    if not sheet_written:\n        df_dummy = pd.DataFrame({\"Message\": [\"No data available for any indicator.\"]})\n        df_dummy.to_excel(writer, sheet_name=\"No_Data\", index=False)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANALYSIS COMPLETED\")\nprint(\"=\"*60)\nprint(f\"USA indicators data saved to: {output_path}\")\n\nprint(\"\\nIndicators processed:\")\nprint(\"1. GDP per capita (constant 2015 US$)\")\nprint(\"2. GDP per person employed (constant 2017 PPP $)\")\nprint(\"3. Individuals using the Internet (% of population)\")\nprint(\"4. Renewable energy consumption (% of total final energy)\")\nprint(\"5. Domestic credit to private sector by banks (% of GDP)\")\nprint(\"6. Adjusted savings: education expenditure (% of GNI)\")\nprint(\"7. Digitalization Index (Fixed telephone, Mobile cellular, Internet usage)\")\nprint(\"8. Labor force, total\")\nprint(\"9. Foreign direct investment, net inflows (% of GDP)\")\nprint(\"10. Employment to population ratio, 15+, total (%)\")\n\nprint(f\"\\nData period: 1990-2023\")\nprint(f\"Country: United States (USA)\")\nprint(f\"Output file: {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fetching just 1 indicator for 46 countries from 1998-2018","metadata":{}},{"cell_type":"code","source":"# Install wbdata if not already installed\n!pip install wbdata\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Define countries with their ISO codes\ncountries = [\n    'ARG',  # Argentina\n    'AUS',  # Australia\n    'AUT',  # Austria\n    'BEL',  # Belgium\n    'BRA',  # Brazil\n    'BGR',  # Bulgaria\n    'CAN',  # Canada\n    'CHN',  # China\n    'CRI',  # Costa Rica\n    'HRV',  # Croatia\n    'CZE',  # Czechia\n    'DNK',  # Denmark\n    'EST',  # Estonia\n    'FIN',  # Finland\n    'FRA',  # France\n    'DEU',  # Germany\n    'GRC',  # Greece\n    'HUN',  # Hungary\n    'ISL',  # Iceland\n    'IND',  # India\n    'IDN',  # Indonesia\n    'IRL',  # Ireland\n    'ISR',  # Israel\n    'ITA',  # Italy\n    'JPN',  # Japan\n    'KOR',  # Korea\n    'LVA',  # Latvia\n    'LTU',  # Lithuania\n    'LUX',  # Luxembourg\n    'MEX',  # Mexico\n    'NLD',  # Netherlands\n    'NZL',  # New Zealand\n    'PER',  # Peru\n    'POL',  # Poland\n    'PRT',  # Portugal\n    'ROU',  # Romania\n    'RUS',  # Russia\n    'SAU',  # Saudi Arabia\n    'SVN',  # Slovenia\n    'ZAF',  # South Africa\n    'ESP',  # Spain\n    'SWE',  # Sweden\n    'CHE',  # Switzerland\n    'TUR',  # Türkiye\n    'GBR',  # United Kingdom\n    'USA'   # United States\n]\n\n# Define the GDP growth indicator\nindicator_code = 'NY.GDP.MKTP.KD.ZG'\nindicator_name = 'GDP growth (annual %)'\n\n# Define the start and end years\nstart_date = datetime(1998, 1, 1)\nend_date = datetime(2018, 12, 31)\n\n# Define output file path\noutput_path = \"/kaggle/working/GDP_Growth_1998_2018.xlsx\"\n\nprint(f\"Fetching data for: {indicator_name}\")\n\ntry:\n    # Fetch data for the given countries and indicator\n    df = wbdata.get_dataframe(\n        {indicator_code: indicator_name}, \n        country=countries, \n        date=(start_date, end_date)\n    ).reset_index()\n    \n    # Check if the dataframe is empty\n    if df.empty:\n        print(f\"No data available for {indicator_name}.\")\n        # Create a dummy dataframe with a message\n        df = pd.DataFrame({\"Message\": [\"No data available for GDP growth.\"]})\n        df.to_excel(output_path, sheet_name=\"No Data\", index=False)\n    else:\n        # Convert 'date' column to datetime objects if it's not already\n        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n            df['date'] = pd.to_datetime(df['date'])\n            \n        # Extract only the year\n        df['year'] = df['date'].dt.year\n        \n        # Drop the 'date' column and sort by country and year\n        df = df.drop(columns=['date']).sort_values(by=['country', 'year'])\n        \n        # Save data to Excel file\n        df.to_excel(output_path, sheet_name='GDP_Growth', index=False)\n        print(f\"Data saved for: {indicator_name}\")\n        \n        # Display basic statistics\n        print(f\"\\nData Summary:\")\n        print(f\"Total records: {len(df)}\")\n        print(f\"Countries with data: {df['country'].nunique()}\")\n        print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")\n        print(f\"Average GDP growth: {df[indicator_name].mean():.2f}%\")\n        \nexcept Exception as e:\n    print(f\"Error fetching data for {indicator_name}: {str(e)}\")\n    # Create error file\n    error_df = pd.DataFrame({\"Error\": [f\"Error fetching data: {str(e)}\"]})\n    error_df.to_excel(output_path, sheet_name=\"Error\", index=False)\n\nprint(f\"\\nGDP growth data saved to {output_path}\")\n\n# Display summary of countries included\nprint(f\"\\nCountries included in the analysis ({len(countries)} total):\")\nfor i, country_code in enumerate(countries, 1):\n    print(f\"{i:2d}. {country_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T06:14:19.203358Z","iopub.execute_input":"2025-06-01T06:14:19.203573Z","iopub.status.idle":"2025-06-01T06:14:43.913257Z","shell.execute_reply.started":"2025-06-01T06:14:19.203553Z","shell.execute_reply":"2025-06-01T06:14:43.912222Z"}},"outputs":[{"name":"stdout","text":"Collecting wbdata\n  Downloading wbdata-1.0.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting appdirs<2.0,>=1.4 (from wbdata)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting backoff<3.0.0,>=2.2.1 (from wbdata)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: cachetools<6.0.0,>=5.3.2 in /usr/local/lib/python3.11/dist-packages (from wbdata) (5.5.2)\nCollecting dateparser<2.0.0,>=1.2.0 (from wbdata)\n  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\nCollecting decorator<6.0.0,>=5.1.1 (from wbdata)\n  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from wbdata) (2.32.3)\nCollecting shelved-cache<0.4.0,>=0.3.1 (from wbdata)\n  Downloading shelved_cache-0.3.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting tabulate<0.9.0,>=0.8.5 (from wbdata)\n  Downloading tabulate-0.8.10-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2.9.0.post0)\nRequirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2025.2)\nRequirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2024.11.6)\nRequirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (5.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.0->wbdata) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.0->wbdata) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.0->wbdata) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.0->wbdata) (2025.4.26)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser<2.0.0,>=1.2.0->wbdata) (1.17.0)\nDownloading wbdata-1.0.0-py3-none-any.whl (18 kB)\nDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\nDownloading shelved_cache-0.3.1-py3-none-any.whl (7.8 kB)\nDownloading tabulate-0.8.10-py3-none-any.whl (29 kB)\nInstalling collected packages: appdirs, tabulate, shelved-cache, decorator, backoff, dateparser, wbdata\n  Attempting uninstall: tabulate\n    Found existing installation: tabulate 0.9.0\n    Uninstalling tabulate-0.9.0:\n      Successfully uninstalled tabulate-0.9.0\n  Attempting uninstall: decorator\n    Found existing installation: decorator 4.4.2\n    Uninstalling decorator-4.4.2:\n      Successfully uninstalled decorator-4.4.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmoviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.2.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nbigframes 1.42.0 requires tabulate>=0.9, but you have tabulate 0.8.10 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 backoff-2.2.1 dateparser-1.2.1 decorator-5.2.1 shelved-cache-0.3.1 tabulate-0.8.10 wbdata-1.0.0\nFetching data for: GDP growth (annual %)\nData saved for: GDP growth (annual %)\n\nData Summary:\nTotal records: 966\nCountries with data: 46\nYear range: 1998 - 2018\nAverage GDP growth: 2.97%\n\nGDP growth data saved to /kaggle/working/GDP_Growth_1998_2018.xlsx\n\nCountries included in the analysis (46 total):\n 1. ARG\n 2. AUS\n 3. AUT\n 4. BEL\n 5. BRA\n 6. BGR\n 7. CAN\n 8. CHN\n 9. CRI\n10. HRV\n11. CZE\n12. DNK\n13. EST\n14. FIN\n15. FRA\n16. DEU\n17. GRC\n18. HUN\n19. ISL\n20. IND\n21. IDN\n22. IRL\n23. ISR\n24. ITA\n25. JPN\n26. KOR\n27. LVA\n28. LTU\n29. LUX\n30. MEX\n31. NLD\n32. NZL\n33. PER\n34. POL\n35. PRT\n36. ROU\n37. RUS\n38. SAU\n39. SVN\n40. ZAF\n41. ESP\n42. SWE\n43. CHE\n44. TUR\n45. GBR\n46. USA\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## fetch  8 educational and economic indicators for the OECD and other specified countries from 1998 to 2018 at once in `one` sheets","metadata":{}},{"cell_type":"code","source":"# Install wbdata if not already installed\n!pip install wbdata\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Define countries with their ISO codes\ncountries = [\n    'AUS',  # Australia\n    'AUT',  # Austria\n    'BEL',  # Belgium\n    'CAN',  # Canada\n    'CHL',  # Chile\n    'COL',  # Colombia\n    'CRI',  # Costa Rica\n    'CZE',  # Czechia\n    'DNK',  # Denmark\n    'EST',  # Estonia\n    'FIN',  # Finland\n    'FRA',  # France\n    'DEU',  # Germany\n    'GRC',  # Greece\n    'HUN',  # Hungary\n    'ISL',  # Iceland\n    'IRL',  # Ireland\n    'ISR',  # Israel\n    'ITA',  # Italy\n    'JPN',  # Japan\n    'KOR',  # Korea\n    'LVA',  # Latvia\n    'LTU',  # Lithuania\n    'LUX',  # Luxembourg\n    'MEX',  # Mexico\n    'NLD',  # Netherlands\n    'NZL',  # New Zealand\n    'NOR',  # Norway\n    'POL',  # Poland\n    'PRT',  # Portugal\n    'SVK',  # Slovak Republic\n    'SVN',  # Slovenia\n    'ESP',  # Spain\n    'SWE',  # Sweden\n    'CHE',  # Switzerland\n    'TUR',  # Türkiye\n    'GBR',  # United Kingdom\n    'USA',  # United States\n    'ARG',  # Argentina\n    'BRA',  # Brazil\n    'BGR',  # Bulgaria\n    'CHN',  # China (People's Republic of)\n    'HRV',  # Croatia\n    'CYP',  # Cyprus\n    'IND',  # India\n    'IDN',  # Indonesia\n    'MLT',  # Malta\n    'PER',  # Peru\n    'ROU',  # Romania\n    'RUS',  # Russia\n    'SAU',  # Saudi Arabia\n    'ZAF'   # South Africa\n]\n\n# Create country ID mapping\ncountry_id_mapping = {country: i+1 for i, country in enumerate(countries)}\n\n# Define indicators with shorter column names for easier analysis\nindicators = {\n    'SE.TER.CUAT.BA.MA.ZS': 'bachelor_male_education',\n    'NV.IND.TOTL.ZS': 'industry_value_added',\n    'BX.KLT.DINV.CD.WD': 'fdi_net',\n    'FD.AST.PRVT.GD.ZS': 'domestic_credit',\n    'NE.TRD.GNFS.ZS': 'trade_gdp',\n    'SL.TLF.ADVN.ZS': 'labor_advanced_education',\n    'SL.TLF.BASC.ZS': 'labor_basic_education',\n    'GB.XPD.RSDV.GD.ZS': 'rd_expenditure'\n}\n\n# Define the start and end years\nstart_date = datetime(1998, 1, 1)\nend_date = datetime(2018, 12, 31)\n\n# Define output file path\noutput_path = \"/kaggle/working/Panel_Data_Education_Economics_1998_2018.xlsx\"\n\nprint(\"Starting data collection...\")\n\n# Initialize an empty list to store all dataframes\nall_data = []\n\n# Fetch data for each indicator\nfor indicator_code, column_name in indicators.items():\n    print(f\"Fetching data for: {column_name}\")\n    \n    try:\n        # Fetch data for the given countries and indicator\n        df = wbdata.get_dataframe(\n            {indicator_code: column_name}, \n            country=countries, \n            date=(start_date, end_date)\n        ).reset_index()\n        \n        # If the dataframe is empty, skip this indicator\n        if df.empty:\n            print(f\"No data available for {column_name}\")\n            continue\n        \n        # Convert 'date' column to datetime objects if it's not already\n        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n            df['date'] = pd.to_datetime(df['date'])\n            \n        # Extract only the year\n        df['year'] = df['date'].dt.year\n        \n        # Drop the 'date' column\n        df = df.drop(columns=['date'])\n        \n        # Add this dataframe to our list\n        all_data.append(df)\n        print(f\"Successfully collected data for: {column_name}\")\n        \n    except Exception as e:\n        print(f\"Error fetching data for {column_name}: {str(e)}\")\n        continue\n\n# Merge all dataframes on country and year\nif all_data:\n    print(\"\\nMerging all indicators into panel data format...\")\n    \n    # Start with the first dataframe\n    panel_data = all_data[0]\n    \n    # Merge with remaining dataframes\n    for df in all_data[1:]:\n        panel_data = pd.merge(panel_data, df, on=['country', 'year'], how='outer')\n    \n    # Add country ID column\n    panel_data['country_id'] = panel_data['country'].map(country_id_mapping)\n    \n    # Reorder columns to have ID, country, year first, then all indicators\n    column_order = ['country_id', 'country', 'year'] + [col for col in panel_data.columns if col not in ['country_id', 'country', 'year']]\n    panel_data = panel_data[column_order]\n    \n    # Sort by country ID and year for proper panel data structure\n    panel_data = panel_data.sort_values(by=['country_id', 'year'])\n    \n    # Reset index\n    panel_data = panel_data.reset_index(drop=True)\n    \n    # Save to Excel\n    with pd.ExcelWriter(output_path) as writer:\n        panel_data.to_excel(writer, sheet_name='Panel_Data', index=False)\n        \n        # Also create a country mapping sheet for reference\n        country_mapping = pd.DataFrame([\n            {'country_id': country_id_mapping[country], 'country_code': country} \n            for country in countries\n        ])\n        country_mapping.to_excel(writer, sheet_name='Country_Mapping', index=False)\n    \n    print(f\"\\nPanel data successfully saved to {output_path}\")\n    print(f\"Data shape: {panel_data.shape[0]} observations, {panel_data.shape[1]} variables\")\n    print(f\"Time period: {panel_data['year'].min()} - {panel_data['year'].max()}\")\n    print(f\"Number of countries: {panel_data['country'].nunique()}\")\n    \n    # Display sample of the data\n    print(f\"\\nSample of the panel data:\")\n    print(panel_data.head(10))\n    \n    # Display summary statistics\n    print(f\"\\nData availability summary:\")\n    for col in panel_data.columns:\n        if col not in ['country_id', 'country', 'year']:\n            non_null_count = panel_data[col].notna().sum()\n            total_count = len(panel_data)\n            print(f\"{col}: {non_null_count}/{total_count} ({non_null_count/total_count*100:.1f}%) observations available\")\n\nelse:\n    print(\"No data was successfully collected for any indicator.\")\n    # Create a dummy file\n    dummy_df = pd.DataFrame({\"Message\": [\"No data available for any indicator.\"]})\n    with pd.ExcelWriter(output_path) as writer:\n        dummy_df.to_excel(writer, sheet_name=\"No_Data\", index=False)\n\nprint(f\"\\nCountries included in the analysis:\")\nfor country_code in countries:\n    country_id = country_id_mapping[country_code]\n    print(f\"ID {country_id:2d}: {country_code}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## fetch  8 educational and economic indicators for the OECD and other specified countries from 1998 to 2018 at once in `different` sheets","metadata":{}},{"cell_type":"code","source":"# Install wbdata if not already installed\n!pip install wbdata\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Define countries with their ISO codes\ncountries = [\n    'AUS',  # Australia\n    'AUT',  # Austria\n    'BEL',  # Belgium\n    'CAN',  # Canada\n    'CHL',  # Chile\n    'COL',  # Colombia\n    'CRI',  # Costa Rica\n    'CZE',  # Czechia\n    'DNK',  # Denmark\n    'EST',  # Estonia\n    'FIN',  # Finland\n    'FRA',  # France\n    'DEU',  # Germany\n    'GRC',  # Greece\n    'HUN',  # Hungary\n    'ISL',  # Iceland\n    'IRL',  # Ireland\n    'ISR',  # Israel\n    'ITA',  # Italy\n    'JPN',  # Japan\n    'KOR',  # Korea\n    'LVA',  # Latvia\n    'LTU',  # Lithuania\n    'LUX',  # Luxembourg\n    'MEX',  # Mexico\n    'NLD',  # Netherlands\n    'NZL',  # New Zealand\n    'NOR',  # Norway\n    'POL',  # Poland\n    'PRT',  # Portugal\n    'SVK',  # Slovak Republic\n    'SVN',  # Slovenia\n    'ESP',  # Spain\n    'SWE',  # Sweden\n    'CHE',  # Switzerland\n    'TUR',  # Türkiye\n    'GBR',  # United Kingdom\n    'USA',  # United States\n    'ARG',  # Argentina\n    'BRA',  # Brazil\n    'BGR',  # Bulgaria\n    'CHN',  # China (People's Republic of)\n    'HRV',  # Croatia\n    'CYP',  # Cyprus\n    'IND',  # India\n    'IDN',  # Indonesia\n    'MLT',  # Malta\n    'PER',  # Peru\n    'ROU',  # Romania\n    'RUS',  # Russia\n    'SAU',  # Saudi Arabia\n    'ZAF'   # South Africa\n]\n\n# Define indicators\nindicators = {\n    'SE.TER.CUAT.BA.MA.ZS': 'Educational attainment, at least Bachelor\\'s or equivalent, population 25+, male (%) (cumulative)',\n    'NV.IND.TOTL.ZS': 'Industry (including construction), value added (% of GDP)',\n    'BX.KLT.DINV.CD.WD': 'Foreign direct investment, net (BoP, current US$)',\n    'FD.AST.PRVT.GD.ZS': 'Domestic credit provided by financial sector (% of GDP)',\n    'NE.TRD.GNFS.ZS': 'Trade (% of GDP)',\n    'SL.TLF.ADVN.ZS': 'Labor force with advanced education (% of total working-age population with advanced education)',\n    'SL.TLF.BASC.ZS': 'Labor force with basic education (% of total working-age population with basic education)',\n    'GB.XPD.RSDV.GD.ZS': 'Research and development expenditure (% of GDP)'\n}\n\n# Define the start and end years\nstart_date = datetime(1998, 1, 1)\nend_date = datetime(2018, 12, 31)\n\n# Define output file path\noutput_path = \"/kaggle/working/Education_Economics_Indicators_1998_2018.xlsx\"\n\n# Create an Excel writer\nwith pd.ExcelWriter(output_path) as writer:\n    sheet_written = False  # Flag to check if at least one sheet is written\n    \n    for indicator_code, indicator_name in indicators.items():\n        print(f\"Fetching data for: {indicator_name}\")\n        \n        try:\n            # Fetch data for the given countries and indicator\n            df = wbdata.get_dataframe(\n                {indicator_code: indicator_name}, \n                country=countries, \n                date=(start_date, end_date)\n            ).reset_index()\n            \n            # If the dataframe is empty, skip writing it\n            if df.empty:\n                print(f\"Skipping {indicator_name} (No data available).\")\n                continue\n            \n            # Convert 'date' column to datetime objects if it's not already\n            if not pd.api.types.is_datetime64_any_dtype(df['date']):\n                df['date'] = pd.to_datetime(df['date'])\n                \n            # Extract only the year\n            df['year'] = df['date'].dt.year\n            \n            # Drop the 'date' column and sort by country and year\n            df = df.drop(columns=['date']).sort_values(by=['country', 'year'])\n            \n            # Create a safe sheet name (Excel sheet names have character limits)\n            sheet_name = indicator_name[:31]  # Limit to 31 characters for Excel compatibility\n            if len(indicator_name) > 31:\n                # Create a shorter, meaningful name\n                if 'Educational attainment' in indicator_name:\n                    sheet_name = 'Bachelor_Male_Education'\n                elif 'Industry' in indicator_name:\n                    sheet_name = 'Industry_Value_Added'\n                elif 'Foreign direct investment' in indicator_name:\n                    sheet_name = 'FDI_Net'\n                elif 'Domestic credit' in indicator_name:\n                    sheet_name = 'Domestic_Credit'\n                elif 'Trade' in indicator_name:\n                    sheet_name = 'Trade_GDP'\n                elif 'advanced education' in indicator_name:\n                    sheet_name = 'Labor_Advanced_Education'\n                elif 'basic education' in indicator_name:\n                    sheet_name = 'Labor_Basic_Education'\n                elif 'Research and development' in indicator_name:\n                    sheet_name = 'RD_Expenditure'\n            \n            # Save data to a sheet\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n            sheet_written = True  # Mark that at least one sheet is written\n            print(f\"Data saved for: {indicator_name}\")\n            \n        except Exception as e:\n            print(f\"Error fetching data for {indicator_name}: {str(e)}\")\n            continue\n    \n    # If no data was written, create a dummy sheet to prevent the error\n    if not sheet_written:\n        df = pd.DataFrame({\"Message\": [\"No data available for any indicator.\"]})\n        df.to_excel(writer, sheet_name=\"No Data\", index=False)\n\nprint(f\"Education and Economics indicators data saved to {output_path}\")\n\n# Optional: Display summary of countries included\nprint(f\"\\nCountries included in the analysis ({len(countries)} total):\")\nfor i, country_code in enumerate(countries, 1):\n    print(f\"{i:2d}. {country_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:58:04.809934Z","iopub.execute_input":"2025-05-22T10:58:04.810285Z","iopub.status.idle":"2025-05-22T10:58:16.601747Z","shell.execute_reply.started":"2025-05-22T10:58:04.810254Z","shell.execute_reply":"2025-05-22T10:58:16.600713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For get 1-52 for 21times (years)","metadata":{}},{"cell_type":"code","source":"for num in range(1, 53):  # From 1 to 52 inclusive\n    for _ in range(21):   # Repeat each number 21 times\n        print(num)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### One tap Go Code","metadata":{}},{"cell_type":"code","source":"# Install wbdata if not already installed\n!pip install wbdata\n\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Define country groups\nregions = {\n    \"Western Europe\": ['AUT', 'BEL', 'FRA', 'DEU', 'LIE', 'LUX', 'MCO', 'NLD', 'CHE', 'GBR', 'IRL'],\n    \"Northern Europe\": ['DNK', 'EST', 'FIN', 'ISL', 'LVA', 'LTU', 'NOR', 'SWE'],\n    \"Southern Europe\": ['ALB', 'AND', 'BIH', 'HRV', 'CYP', 'GRC', 'ITA', 'MLT', 'MNE', 'MKD', 'PRT', 'SMR', 'SRB', 'SVN', 'ESP'],\n    \"Central Europe\": ['AUT', 'CZE', 'DEU', 'HUN', 'LIE', 'POL', 'SVK', 'SVN', 'CHE'],\n    \"Southeastern Europe\": ['ALB', 'BIH', 'BGR', 'HRV', 'MNE', 'MKD', 'ROU', 'SRB', 'SVN', 'GRC']\n}\n\n# Indicator for Energy use per capita\nindicator = {'EG.FEC.RNEW.ZS': 'Renewable energy consumption (% of total final energy consumption)'}\n\n# Define the start and end years\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Define output file path for Kaggle\noutput_path = \"/kaggle/working/Renewable energy consumption_2010_2019.xlsx\"\n\n# Create an Excel writer\nwith pd.ExcelWriter(output_path) as writer:\n    sheet_written = False  # Flag to check if at least one sheet is written\n    for region, countries in regions.items():\n        # Fetch data for the given countries\n        df = wbdata.get_dataframe(\n            indicator, \n            country=countries, \n            date=(start_date, end_date)\n        ).reset_index()\n        \n        # If the dataframe is empty, skip writing it\n        if df.empty:\n            print(f\"Skipping {region} (No data available).\")\n            continue\n        \n        # Convert 'date' column to datetime objects if it's not already\n        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n            df['date'] = pd.to_datetime(df['date'])\n            \n        # Extract only the year\n        df['year'] = df['date'].dt.year\n        \n        # Drop the 'date' column and sort by 'year'\n        df = df.drop(columns=['date']).sort_values(by=['year'])\n        \n        # Save data to a sheet\n        df.to_excel(writer, sheet_name=region, index=False)\n        sheet_written = True  # Mark that at least one sheet is written\n\n    # If no data was written, create a dummy sheet to prevent the error\n    if not sheet_written:\n        df = pd.DataFrame({\"Message\": [\"No data available for any region.\"]})\n        df.to_excel(writer, sheet_name=\"No Data\", index=False)\n\nprint(f\"Energy use per capita data saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:39:30.935435Z","iopub.execute_input":"2025-02-09T08:39:30.935781Z","iopub.status.idle":"2025-02-09T08:40:05.664957Z","shell.execute_reply.started":"2025-02-09T08:39:30.935755Z","shell.execute_reply":"2025-02-09T08:40:05.664041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Concept 1","metadata":{}},{"cell_type":"code","source":"import wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# List of countries to fetch data for (using ISO 3166-1 alpha-3 country codes)\ncountries = ['AUT', 'BEL', 'FRA', 'DEU', 'LIE', 'LUX', 'MCO', 'NLD', 'CHE', 'GBR', 'IRL']\n\n# Indicator for Energy use (kg of oil equivalent per capita)\nindicator = {'EG.USE.PCAP.KG.OE': 'Energy use (kg of oil equivalent per capita)'}\n\n# Define the start and end dates\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Fetch the data using wbdata\ndf = wbdata.get_dataframe(\n    indicator, \n    country=countries, \n    date=(start_date, end_date)  # Use 'date' instead of 'data_date'\n)\n\n# Reset the index to have separate columns for 'country' and 'date'\ndf = df.reset_index()\n\n# Extract only the year from the 'date' column\ndf['year'] = pd.to_datetime(df['date']).dt.year\n\n# Drop the 'date' column and sort by 'year'\ndf = df.drop(columns=['date'])\ndf = df.sort_values(by=['year'])\n\n# Display the data\nprint(df)\n\n# Save the output as an Excel file for Kaggle output\n#df.to_excel('/kaggle/working/Energy_use_per_capita_2010_2019.xlsx')  # Save to Kaggle output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:51:01.767001Z","iopub.execute_input":"2025-02-03T09:51:01.767407Z","iopub.status.idle":"2025-02-03T09:51:02.840047Z","shell.execute_reply.started":"2025-02-03T09:51:01.767375Z","shell.execute_reply":"2025-02-03T09:51:02.838988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experimental one tap code","metadata":{}},{"cell_type":"code","source":"# Install wbdata if not already installed\n!pip install wbdata\n\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Define country groups\nregions = {\n    \"Western Europe\": ['AUT', 'BEL', 'FRA', 'DEU', 'LIE', 'LUX', 'MCO', 'NLD', 'CHE', 'GBR', 'IRL'],\n    \"Northern Europe\": ['DNK', 'EST', 'FIN', 'ISL', 'LVA', 'LTU', 'NOR', 'SWE'],\n    \"Southern Europe\": ['ALB', 'AND', 'BIH', 'HRV', 'CYP', 'GRC', 'ITA', 'MLT', 'MNE', 'MKD', 'PRT', 'SMR', 'SRB', 'SVN', 'ESP'],\n    \"Central Europe\": ['AUT', 'CZE', 'DEU', 'HUN', 'LIE', 'POL', 'SVK', 'SVN', 'CHE'],\n    \"Southeastern Europe\": ['ALB', 'BIH', 'BGR', 'HRV', 'MNE', 'MKD', 'ROU', 'SRB', 'SVN', 'GRC']\n}\n\n# Indicator for Energy use per capita\nindicator = {'EG.USE.PCAP.KG.OE': 'Energy use (kg of oil equivalent per capita)'}\n\n# Define the start and end years\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Define output file path for Kaggle\noutput_path = \"/kaggle/working/Energy_Use_Per_Capita_2010_2019.xlsx\"\n\n# Create an Excel writer\nwith pd.ExcelWriter(output_path) as writer:\n    sheet_written = False  # Flag to check if at least one sheet is written\n    for region, countries in regions.items():\n        # Fetch data for the given countries\n        df = wbdata.get_dataframe(\n            indicator, \n            country=countries, \n            date=(start_date, end_date)\n        ).reset_index()\n        \n        # If the dataframe is empty, skip writing it\n        if df.empty:\n            print(f\"Skipping {region} (No data available).\")\n            continue\n        \n        # Convert 'date' column to datetime objects if it's not already\n        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n            df['date'] = pd.to_datetime(df['date'])\n            \n        # Extract only the year\n        df['year'] = df['date'].dt.year\n        \n        # Drop the 'date' column and sort by 'year'\n        df = df.drop(columns=['date']).sort_values(by=['year'])\n        \n        # Save data to a sheet\n        df.to_excel(writer, sheet_name=region, index=False)\n        sheet_written = True  # Mark that at least one sheet is written\n\n    # If no data was written, create a dummy sheet to prevent the error\n    if not sheet_written:\n        df = pd.DataFrame({\"Message\": [\"No data available for any region.\"]})\n        df.to_excel(writer, sheet_name=\"No Data\", index=False)\n\nprint(f\"Energy use per capita data saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T10:17:14.424188Z","iopub.execute_input":"2025-02-03T10:17:14.424601Z","iopub.status.idle":"2025-02-03T10:17:18.515865Z","shell.execute_reply.started":"2025-02-03T10:17:14.42457Z","shell.execute_reply":"2025-02-03T10:17:18.514875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prototype code","metadata":{}},{"cell_type":"code","source":"import wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# List of countries to fetch data for (using ISO 3166-1 alpha-3 country codes)\ncountries = ['DNK', 'EST', 'FIN', 'ISL', 'IRL', 'LVA', 'LTU', 'NOR', 'SWE']\n\n# Indicator for trade (% of GDP)\nindicator = {'NE.TRD.GNFS.ZS': 'Trade (% of GDP)'}\n\n# Define the start and end dates\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Fetch the data using wbdata\ndf = wbdata.get_dataframe(\n    indicator, \n    country=countries, \n    date=(start_date, end_date)  # Use 'date' instead of 'data_date'\n)\n\n# Reset the index to have separate columns for 'country' and 'date'\ndf = df.reset_index()\n\n# Convert the 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Set the index back to 'date' if desired\ndf = df.set_index('date')\n\n# Display the data\nprint(df)\n\n# Save the output as an Excel file for Kaggle output\ndf.to_excel('/kaggle/working/Trade_percent_of_GDP_2010_2019.xlsx')  # Save to Kaggle output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:09:38.756964Z","iopub.execute_input":"2025-02-01T10:09:38.757436Z","iopub.status.idle":"2025-02-01T10:09:39.789813Z","shell.execute_reply.started":"2025-02-01T10:09:38.757396Z","shell.execute_reply":"2025-02-01T10:09:39.788339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Code","metadata":{}},{"cell_type":"markdown","source":"### Trade openness for western european countries","metadata":{}},{"cell_type":"code","source":"import wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# List of countries to fetch data for (using ISO 3166-1 alpha-3 country codes)\ncountries = ['AUT', 'BEL', 'FRA', 'DEU', 'LIE', 'LUX', 'MCO', 'NLD', 'CHE', 'GBR', 'IRL']\n\n# Indicator for trade (% of GDP)\nindicator = {'NE.TRD.GNFS.ZS': 'Trade (% of GDP)'}\n\n# Define the start and end dates\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Fetch the data using wbdata\ndf = wbdata.get_dataframe(\n    indicator, \n    country=countries, \n    date=(start_date, end_date)  # Use 'date' instead of 'data_date'\n)\n\n# Reset the index to have separate columns for 'country' and 'date'\ndf = df.reset_index()\n\n# Extract only the year from the 'date' column\ndf['year'] = pd.to_datetime(df['date']).dt.year\n\n# Drop the 'date' column and sort by 'year'\ndf = df.drop(columns=['date'])\ndf = df.sort_values(by=['year'])\n\n# Display the data\nprint(df)\n\n# Save the output as an Excel file for Kaggle output\ndf.to_excel('/kaggle/working/F Trade_percent_of_GDP_2010_2019.xlsx')  # Save to Kaggle output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:39:56.630352Z","iopub.execute_input":"2025-05-31T10:39:56.630671Z","iopub.status.idle":"2025-05-31T10:39:56.84394Z","shell.execute_reply.started":"2025-05-31T10:39:56.630646Z","shell.execute_reply":"2025-05-31T10:39:56.842409Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/743413512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Fetch the data using wbdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m df = wbdata.get_dataframe(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-119>\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(self, indicators, country, date, freq, source, parse_dates, keep_levels, skip_cache)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mneeds_pandas\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f.__name__} requires pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(self, indicators, country, date, freq, source, parse_dates, keep_levels, skip_cache)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \"\"\"\n\u001b[1;32m    542\u001b[0m         df = DataFrame(\n\u001b[0;32m--> 543\u001b[0;31m             serieses={\n\u001b[0m\u001b[1;32m    544\u001b[0m                 name: self.get_series(\n\u001b[1;32m    545\u001b[0m                     \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    542\u001b[0m         df = DataFrame(\n\u001b[1;32m    543\u001b[0m             serieses={\n\u001b[0;32m--> 544\u001b[0;31m                 name: self.get_series(\n\u001b[0m\u001b[1;32m    545\u001b[0m                     \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-118>\u001b[0m in \u001b[0;36mget_series\u001b[0;34m(self, indicator, country, date, freq, source, parse_dates, name, keep_levels, skip_cache)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mneeds_pandas\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f.__name__} requires pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mget_series\u001b[0;34m(self, indicator, country, date, freq, source, parse_dates, name, keep_levels, skip_cache)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0monly\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mone\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \"\"\"\n\u001b[0;32m--> 469\u001b[0;31m         raw_data = self.get_data(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, indicator, country, date, freq, source, parse_dates, skip_cache)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_row_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/fetcher.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, url, params, skip_cache)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             response = self._get_response(\n\u001b[0m\u001b[1;32m    156\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/fetcher.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(self, url, params, skip_cache)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mParsedResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     def fetch(\n","\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"],"ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"### Trade openness for `Southern European` countries","metadata":{}},{"cell_type":"code","source":"import wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# List of countries to fetch data for (using ISO 3166-1 alpha-3 country codes)\ncountries = ['ALB', 'AND', 'BIH', 'HRV', 'CYP', 'GRC', 'ITA', 'MLT', 'MNE', 'MKD', 'PRT', 'SMR', 'SRB', 'SVN', 'ESP']\n\n# Indicator for trade (% of GDP)\nindicator = {'NE.TRD.GNFS.ZS': 'Trade (% of GDP)'}\n\n# Define the start and end dates\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Fetch the data using wbdata\ndf = wbdata.get_dataframe(\n    indicator, \n    country=countries, \n    date=(start_date, end_date)  # Use 'date' instead of 'data_date'\n)\n\n# Reset the index to have separate columns for 'country' and 'date'\ndf = df.reset_index()\n\n# Extract only the year from the 'date' column\ndf['year'] = pd.to_datetime(df['date']).dt.year\n\n# Drop the 'date' column and sort by 'year'\ndf = df.drop(columns=['date'])\ndf = df.sort_values(by=['year'])\n\n# Display the data\nprint(df)\n\n# Save the output as an Excel file for Kaggle output\ndf.to_excel('/kaggle/working/SE Trade_percent_of_GDP_2010_2019.xlsx')  # Save to Kaggle output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:39:23.086047Z","iopub.execute_input":"2025-05-31T10:39:23.086796Z","iopub.status.idle":"2025-05-31T10:39:23.322958Z","shell.execute_reply.started":"2025-05-31T10:39:23.086753Z","shell.execute_reply":"2025-05-31T10:39:23.321587Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3662097092.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Fetch the data using wbdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m df = wbdata.get_dataframe(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-119>\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(self, indicators, country, date, freq, source, parse_dates, keep_levels, skip_cache)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mneeds_pandas\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f.__name__} requires pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(self, indicators, country, date, freq, source, parse_dates, keep_levels, skip_cache)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \"\"\"\n\u001b[1;32m    542\u001b[0m         df = DataFrame(\n\u001b[0;32m--> 543\u001b[0;31m             serieses={\n\u001b[0m\u001b[1;32m    544\u001b[0m                 name: self.get_series(\n\u001b[1;32m    545\u001b[0m                     \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    542\u001b[0m         df = DataFrame(\n\u001b[1;32m    543\u001b[0m             serieses={\n\u001b[0;32m--> 544\u001b[0;31m                 name: self.get_series(\n\u001b[0m\u001b[1;32m    545\u001b[0m                     \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-118>\u001b[0m in \u001b[0;36mget_series\u001b[0;34m(self, indicator, country, date, freq, source, parse_dates, name, keep_levels, skip_cache)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mneeds_pandas\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f.__name__} requires pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mget_series\u001b[0;34m(self, indicator, country, date, freq, source, parse_dates, name, keep_levels, skip_cache)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0monly\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mone\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \"\"\"\n\u001b[0;32m--> 469\u001b[0;31m         raw_data = self.get_data(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/client.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, indicator, country, date, freq, source, parse_dates, skip_cache)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_row_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/fetcher.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, url, params, skip_cache)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             response = self._get_response(\n\u001b[0m\u001b[1;32m    156\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wbdata/fetcher.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(self, url, params, skip_cache)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mParsedResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     def fetch(\n","\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"],"ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"### Trade openness for `Central Europe` countries","metadata":{}},{"cell_type":"code","source":"import wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# List of countries to fetch data for (using ISO 3166-1 alpha-3 country codes)\ncountries = ['AUT', 'CZE', 'DEU', 'HUN', 'LIE', 'POL', 'SVK', 'SVN', 'CHE']\n\n# Indicator for trade (% of GDP)\nindicator = {'NE.TRD.GNFS.ZS': 'Trade (% of GDP)'}\n\n# Define the start and end dates\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Fetch the data using wbdata\ndf = wbdata.get_dataframe(\n    indicator, \n    country=countries, \n    date=(start_date, end_date)  # Use 'date' instead of 'data_date'\n)\n\n# Reset the index to have separate columns for 'country' and 'date'\ndf = df.reset_index()\n\n# Extract only the year from the 'date' column\ndf['year'] = pd.to_datetime(df['date']).dt.year\n\n# Drop the 'date' column and sort by 'year'\ndf = df.drop(columns=['date'])\ndf = df.sort_values(by=['year'])\n\n# Display the data\nprint(df)\n\n# Save the output as an Excel file for Kaggle output\ndf.to_excel('/kaggle/working/CE Trade_percent_of_GDP_2010_2019.xlsx')  # Save to Kaggle output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T11:13:59.612818Z","iopub.execute_input":"2025-02-01T11:13:59.613157Z","iopub.status.idle":"2025-02-01T11:13:59.655096Z","shell.execute_reply.started":"2025-02-01T11:13:59.61313Z","shell.execute_reply":"2025-02-01T11:13:59.654094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trade openness for `Southeastern Europe (Balkans)` countries","metadata":{}},{"cell_type":"code","source":"import wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# List of countries to fetch data for (using ISO 3166-1 alpha-3 country codes)\ncountries = ['ALB', 'BIH', 'BGR', 'HRV', 'XKX', 'MNE', 'MKD', 'ROU', 'SRB', 'SVN', 'GRC']\n\n# Indicator for trade (% of GDP)\nindicator = {'NE.TRD.GNFS.ZS': 'Trade (% of GDP)'}\n\n# Define the start and end dates\nstart_date = datetime(2010, 1, 1)\nend_date = datetime(2019, 12, 31)\n\n# Fetch the data using wbdata\ndf = wbdata.get_dataframe(\n    indicator, \n    country=countries, \n    date=(start_date, end_date)  # Use 'date' instead of 'data_date'\n)\n\n# Reset the index to have separate columns for 'country' and 'date'\ndf = df.reset_index()\n\n# Extract only the year from the 'date' column\ndf['year'] = pd.to_datetime(df['date']).dt.year\n\n# Drop the 'date' column and sort by 'year'\ndf = df.drop(columns=['date'])\ndf = df.sort_values(by=['year'])\n\n# Display the data\nprint(df)\n\n# Save the output as an Excel file for Kaggle output\ndf.to_excel('/kaggle/working/SCE Trade_percent_of_GDP_2010_2019.xlsx')  # Save to Kaggle output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T11:22:52.94906Z","iopub.execute_input":"2025-02-01T11:22:52.949437Z","iopub.status.idle":"2025-02-01T11:22:53.365763Z","shell.execute_reply.started":"2025-02-01T11:22:52.949404Z","shell.execute_reply":"2025-02-01T11:22:53.364788Z"}},"outputs":[],"execution_count":null}]}